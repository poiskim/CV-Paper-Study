{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-23 06:32:32.257538: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-23 06:32:32.268386: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-23 06:32:32.268420: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-23 06:32:32.275764: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-23 06:32:32.800505: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow는 GPU를 사용할 수 있습니다.\n",
      "사용 가능한 GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-23 06:32:36.553312: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-23 06:32:36.569648: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-23 06:32:36.569689: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "# GPU 장치 목록 확인\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"TensorFlow는 GPU를 사용할 수 있습니다.\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"사용 가능한 GPU: {gpu}\")\n",
    "\n",
    "    # gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "    # print(gpus)\n",
    "    # if gpus:\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"TensorFlow는 GPU를 사용할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 데이터셋 로드\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "x_train = x_train.reshape(-1, 32, 32, 3).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 32, 32, 3).astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train, 100)\n",
    "y_test = to_categorical(y_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (224, 224)\n",
    "batch_size = 64\n",
    "\n",
    "def process_batch(images, target_size):\n",
    "    resized_images = []\n",
    "    for img in images:\n",
    "        resized_image = tf.image.resize(img, target_size)\n",
    "        resized_images.append(resized_image)\n",
    "    return tf.stack(resized_images)\n",
    "\n",
    "# 메모리에 한 번에 올리지 않고 배치 단위로 처리\n",
    "def resize_in_batches(images, target_size, batch_size):\n",
    "    num_batches = len(images) // batch_size\n",
    "    resized_images = []\n",
    "    for i in range(num_batches):\n",
    "        batch = images[i * batch_size:(i + 1) * batch_size]\n",
    "        resized_images.extend(process_batch(batch, target_size))\n",
    "    return np.array(resized_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-23 06:32:46.453576: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-23 06:32:46.453650: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-23 06:32:46.453671: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-23 06:32:46.583477: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-23 06:32:46.583529: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-23 06:32:46.583536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-08-23 06:32:46.583567: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-23 06:32:46.583590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5592 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:2b:00.0, compute capability: 8.6\n",
      "2024-08-23 06:32:57.027284: W external/local_tsl/tsl/framework/bfc_allocator.cc:487] Allocator (GPU_0_bfc) ran out of memory trying to allocate 28.04GiB (rounded to 30105600000)requested by op ResizeBilinear\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2024-08-23 06:32:57.027314: I external/local_tsl/tsl/framework/bfc_allocator.cc:1044] BFCAllocator dump for GPU_0_bfc\n",
      "2024-08-23 06:32:57.027321: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (256): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027325: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027329: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027331: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027333: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027335: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027337: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027339: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027341: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027343: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027345: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027347: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027349: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027351: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027353: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027355: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027357: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027359: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027361: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027364: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027367: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (268435456): \tTotal Chunks: 2, Chunks in use: 1. 5.46GiB allocated for chunks. 585.94MiB in use in bin. 585.94MiB client-requested in use in bin.\n",
      "2024-08-23 06:32:57.027370: I external/local_tsl/tsl/framework/bfc_allocator.cc:1067] Bin for 28.04GiB was 256.00MiB, Chunk State: \n",
      "2024-08-23 06:32:57.027374: I external/local_tsl/tsl/framework/bfc_allocator.cc:1073]   Size: 4.89GiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 1.2KiB | Requested Size: 1.0KiB | in_use: 1 | bin_num: -1\n",
      "2024-08-23 06:32:57.027376: I external/local_tsl/tsl/framework/bfc_allocator.cc:1080] Next region of size 5863636992\n",
      "2024-08-23 06:32:57.027380: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 707c00000 of size 614400000 next 1\n",
      "2024-08-23 06:32:57.027382: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 72c5f0000 of size 1280 next 2\n",
      "2024-08-23 06:32:57.027384: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] Free  at 72c5f0500 of size 5249235712 next 18446744073709551615\n",
      "2024-08-23 06:32:57.027386: I external/local_tsl/tsl/framework/bfc_allocator.cc:1105]      Summary of in-use Chunks by size: \n",
      "2024-08-23 06:32:57.027389: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2024-08-23 06:32:57.027391: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 1 Chunks of size 614400000 totalling 585.94MiB\n",
      "2024-08-23 06:32:57.027393: I external/local_tsl/tsl/framework/bfc_allocator.cc:1112] Sum Total of in-use chunks: 585.94MiB\n",
      "2024-08-23 06:32:57.027396: I external/local_tsl/tsl/framework/bfc_allocator.cc:1114] Total bytes in pool: 5863636992 memory_limit_: 5863636992 available bytes: 0 curr_region_allocation_bytes_: 11727273984\n",
      "2024-08-23 06:32:57.027401: I external/local_tsl/tsl/framework/bfc_allocator.cc:1119] Stats: \n",
      "Limit:                      5863636992\n",
      "InUse:                       614401280\n",
      "MaxInUse:                    614401280\n",
      "NumAllocs:                           2\n",
      "MaxAllocSize:                614400000\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-08-23 06:32:57.027404: W external/local_tsl/tsl/framework/bfc_allocator.cc:499] ***********_________________________________________________________________________________________\n",
      "2024-08-23 06:32:57.027422: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at image_resizer_state.h:154 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[50000,224,224,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2024-08-23 06:32:57.027434: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[50000,224,224,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__ResizeBilinear_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[50000,224,224,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ResizeBilinear] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m resized_x_train \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m resized_x_test \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mresize(x_test, (\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__ResizeBilinear_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[50000,224,224,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ResizeBilinear] name: "
     ]
    }
   ],
   "source": [
    "resized_x_train = tf.image.resize(x_train, (224, 224))\n",
    "resized_x_test = tf.image.resize(x_test, (224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
